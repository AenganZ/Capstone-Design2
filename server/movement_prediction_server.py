import os
import json
import torch
import re
from typing import Dict, Any, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

BASE_MODEL_NAME = "K-intelligence/Midm-2.0-Mini-Instruct"


class PredictionRequest(BaseModel):
    person: Dict[str, Any]
    environment: Dict[str, Dict[str, Any]]
    lat: float
    lon: float


class PredictionResponse(BaseModel):
    success: bool
    prediction: Optional[Dict[str, Dict[str, Any]]] = None
    error: Optional[str] = None


class MovementPredictor:
    def __init__(self, model_path: str = "./movement_predictor_model_v2"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {self.device}")
        self.model = None
        self.tokenizer = None
        self.load_model()

    def load_model(self):
        try:
            print(f"ğŸ“¦ ë² ì´ìŠ¤ Midm-2.0 ë¡œë”©")
            
            self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL_NAME,
                quantization_config=bnb_config,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16,
            )
            
            self.model.eval()
            print("âœ… ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {e}")
            raise

    def create_prompt(self, person: Dict, environment: Dict) -> str:
        p_cat = person.get('category', 'ê¸°íƒ€')
        p_age = person.get('age', 30)
        p_sex = person.get('sex', 'M')
        sex_kor = "ë‚¨ì„±" if p_sex == "M" else "ì—¬ì„±"
        
        # í™˜ê²½ ê°•ì¡°
        env_lines = []
        for d in ["north", "east", "south", "west"]:
            e = environment.get(d, {})
            parts = []
            
            road = e.get('road_type', 'ì—†ìŒ')
            if road != 'ì—†ìŒ':
                parts.append(f"ë„ë¡œ:{road}")
            
            land = e.get('land_use', 'ì—†ìŒ')
            if land != 'ì—†ìŒ':
                parts.append(f"í† ì§€:{land}")
            
            poi = e.get('poi', [])
            if poi and poi != ['ì—†ìŒ']:
                parts.append(f"POI:{','.join(poi)}")
            
            hazard = e.get('hazard', [])
            if hazard and hazard != ['ì—†ìŒ']:
                parts.append(f"âš ï¸ìœ„í—˜:{','.join(hazard)}")
            
            env_lines.append(f"â€¢ {d.upper()}: {' | '.join(parts) if parts else 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ'}")
        
        env_str = "\n".join(env_lines)
        
        prompt = f"""ì‹¤ì¢…ì ì´ë™ ê²½ë¡œ ì˜ˆì¸¡ (í™˜ê²½ ë¶„ì„ í•„ìˆ˜!)

ã€ì‹¤ì¢…ìã€‘
ë¶„ë¥˜: {p_cat} / ë‚˜ì´: {p_age}ì„¸ / ì„±ë³„: {sex_kor}

ã€ê° ë°©í–¥ í™˜ê²½ - ë°˜ë“œì‹œ ê³ ë ¤í•  ê²ƒã€‘
{env_str}

ã€ë¶„ì„ ê·œì¹™ã€‘
âœ“ ê³µì›/í•™êµ â†’ ì•„ë™Â·ê³ ë ¹ì ì„ í˜¸
âœ“ í•˜ì²œ/ê¸‰ê²½ì‚¬/ëŒ€í˜•êµì°¨ë¡œ â†’ ìœ„í—˜ íšŒí”¼
âœ“ ëŒ€ë¡œ/ë²„ìŠ¤/ì§€í•˜ì²  â†’ ê°€ì¶œ ì„±ì¸ ì„ í˜¸
âœ“ ìƒì—…ì§€ì—­ â†’ ê°€ì¶œ ì„ í˜¸, ì£¼ê±°ì§€ì—­ â†’ ê°€ì¶œ íšŒí”¼

ë°˜ë“œì‹œ ìœ„ í™˜ê²½ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì—¬ JSON ì¶œë ¥:
{{"north":{{"prob":ìˆ«ì,"reason":"í™˜ê²½ê·¼ê±°"}},"east":{{"prob":ìˆ«ì,"reason":"í™˜ê²½ê·¼ê±°"}},"south":{{"prob":ìˆ«ì,"reason":"í™˜ê²½ê·¼ê±°"}},"west":{{"prob":ìˆ«ì,"reason":"í™˜ê²½ê·¼ê±°"}}}}"""
        
        return prompt

    def predict(self, person: Dict, environment: Dict) -> Dict[str, Dict[str, Any]]:
        if self.model is None:
            raise Exception("ëª¨ë¸ ë¯¸ë¡œë“œ")

        # í™˜ê²½ ë™ì¼ì„± ì²´í¬
        envs = [environment.get(d, {}) for d in ["north", "east", "south", "west"]]
        
        all_same = all(
            env.get('road_type') == envs[0].get('road_type') and
            env.get('land_use') == envs[0].get('land_use') and
            env.get('poi') == envs[0].get('poi') and
            env.get('hazard') == envs[0].get('hazard')
            for env in envs
        )
        
        if all_same:
            print("âš ï¸  í™˜ê²½ ë™ì¼ â†’ ê· ë“± ë¶„í¬")
            return {
                "north": {"prob": 25.0, "reason": "ì‚¬ë°© í™˜ê²½ ë™ì¼"},
                "east": {"prob": 25.0, "reason": "ì‚¬ë°© í™˜ê²½ ë™ì¼"},
                "south": {"prob": 25.0, "reason": "ì‚¬ë°© í™˜ê²½ ë™ì¼"},
                "west": {"prob": 25.0, "reason": "ì‚¬ë°© í™˜ê²½ ë™ì¼"},
            }

        print("\n=== í™˜ê²½ ì…ë ¥ (ì°¨ì´ ìˆìŒ) ===")
        for d in ["north", "east", "south", "west"]:
            print(f"{d}: {environment.get(d, {})}")
        print("=" * 50)

        for attempt in range(2):
            try:
                prompt = self.create_prompt(person, environment)
                
                messages = [
                    {"role": "system", "content": "í™˜ê²½ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì—¬ JSON ì¶œë ¥"},
                    {"role": "user", "content": prompt},
                ]

                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                enc = self.tokenizer(text, return_tensors="pt", return_token_type_ids=False)
                inputs = {
                    "input_ids": enc["input_ids"].to(self.device),
                    "attention_mask": enc["attention_mask"].to(self.device),
                }

                print(f"ğŸ¤– ì¶”ë¡  {attempt+1}/2")

                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids=inputs["input_ids"],
                        attention_mask=inputs["attention_mask"],
                        max_new_tokens=600,
                        temperature=0.5,
                        top_p=0.9,
                        do_sample=True,
                        pad_token_id=self.tokenizer.pad_token_id,
                    )

                response_text = self.tokenizer.decode(
                    outputs[0][inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True,
                )

                print(f"ğŸ“ ì‘ë‹µ: {response_text}")
                
                prediction = self.parse_response(response_text)
                
                if prediction:
                    print(f"âœ… ì„±ê³µ")
                    for d in ["north", "east", "south", "west"]:
                        print(f"   {d}: {prediction[d]['prob']}% - {prediction[d]['reason'][:30]}")
                    return prediction
                        
            except Exception as e:
                print(f"âŒ {e}")
        
        raise Exception("2ë²ˆ ì‹¤íŒ¨")

    def parse_response(self, text: str) -> Optional[Dict]:
        text = text.strip().replace("'", '"')
        text = re.sub(r'```[\w]*', '', text).replace('```', '')
        
        # ê°œë³„ ì¶”ì¶œ (ë” ì•ˆì „)
        result = {}
        for d in ["north", "east", "south", "west"]:
            prob_match = re.search(rf'"{d}"[^}}]*?"prob"[^:]*?:\s*([\d.]+)', text)
            reason_match = re.search(rf'"{d}"[^}}]*?"reason"[^:]*?:\s*"([^"]*)"', text)
            
            if prob_match:
                prob_val = float(prob_match.group(1))
                if prob_val < 1.5:  # 0~1 ì‚¬ì´ë©´ 100 ê³±í•˜ê¸°
                    prob_val *= 100
                
                result[d] = {
                    "prob": int(prob_val),
                    "reason": reason_match.group(1) if reason_match else "ë¶„ì„ ê²°ê³¼"
                }
        
        # 4ë°©í–¥ ì•ˆ ì±„ì›Œì¡Œìœ¼ë©´ ê· ë“± ë¶„ë°°
        if len(result) < 4:
            missing = [d for d in ["north", "east", "south", "west"] if d not in result]
            for d in missing:
                result[d] = {"prob": 25, "reason": "ì •ë³´ ë¶€ì¡±"}
        
        return self.normalize(result) if len(result) == 4 else None
    
    def normalize(self, pred: Dict) -> Dict:
        total = sum(pred[d]["prob"] for d in ["north", "east", "south", "west"])
        if total == 0:
            for d in ["north", "east", "south", "west"]:
                pred[d]["prob"] = 25.0
        else:
            for d in ["north", "east", "south", "west"]:
                pred[d]["prob"] = round(pred[d]["prob"] * 100.0 / total, 1)
        
        for d in ["north", "east", "south", "west"]:
            if "reason" not in pred[d] or not pred[d]["reason"]:
                pred[d]["reason"] = "ë¶„ì„"

            reason = pred[d]["reason"]

            reason = reason.replace('. ', '.\n')
            reason = reason.replace('ã€‚ ', 'ã€‚\n')

            if len(reason) > 200:
                reason = reason[:197] + "..."

            pred[d]["reason"] = reason
        
        return pred


app = FastAPI(title="ì´ë™ ê²½ë¡œ ì˜ˆì¸¡ ì„œë²„")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

predictor = MovementPredictor()


@app.post("/api/predict_movement", response_model=PredictionResponse)
async def predict_movement(request: PredictionRequest):
    try:
        prediction = predictor.predict(request.person, request.environment)
        return PredictionResponse(success=True, prediction=prediction)
    except Exception as e:
        print(f"ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "model_loaded": predictor.model is not None}


@app.get("/")
async def root():
    return {"message": "ì´ë™ ê²½ë¡œ ì˜ˆì¸¡ ì„œë²„", "port": 8002}


if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ì„œë²„ ì‹œì‘ (í¬íŠ¸ 8002)")
    uvicorn.run(app, host="0.0.0.0", port=8002, reload=False)